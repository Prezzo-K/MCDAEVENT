{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMk2txGxEDG2CtgcdW9SXKv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Prezzo-K/MCDAEVENT/blob/main/Speech_To_Text_Prototype.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install OpenAI's Whisper"
      ],
      "metadata": {
        "id": "Ei54rphsPbBA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "09oKUZQlRlBo",
        "outputId": "06e5d4ca-45f7-4572-fdb6-1da350e05e12"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai-whisper\n",
            "  Downloading openai-whisper-20240930.tar.gz (800 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/800.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m800.5/800.5 kB\u001b[0m \u001b[31m26.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (0.60.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (2.0.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (2.6.0+cu124)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (4.67.1)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (10.6.0)\n",
            "Collecting tiktoken (from openai-whisper)\n",
            "  Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: triton>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (3.2.0)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba->openai-whisper) (0.43.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken->openai-whisper) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken->openai-whisper) (2.32.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (2024.10.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch->openai-whisper)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch->openai-whisper)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch->openai-whisper)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch->openai-whisper)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch->openai-whisper)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch->openai-whisper)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch->openai-whisper)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch->openai-whisper)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch->openai-whisper)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch->openai-whisper)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->openai-whisper) (1.3.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (2025.1.31)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->openai-whisper) (3.0.2)\n",
            "Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m48.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m106.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m87.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m49.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m84.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: openai-whisper\n",
            "  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for openai-whisper: filename=openai_whisper-20240930-py3-none-any.whl size=803375 sha256=6bca943ffe6c06316272c63a1e640d56c1c5346c0de13df8c4e8255c251fcf20\n",
            "  Stored in directory: /root/.cache/pip/wheels/2f/f2/ce/6eb23db4091d026238ce76703bd66da60b969d70bcc81d5d3a\n",
            "Successfully built openai-whisper\n",
            "Installing collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, tiktoken, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, openai-whisper\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 openai-whisper-20240930 tiktoken-0.9.0\n"
          ]
        }
      ],
      "source": [
        "pip install -q -U openai-whisper"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo apt update && sudo apt install ffmpeg"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gTZCkmBdTVfz",
        "outputId": "cce8e9e3-f635-4532-dba6-7e6433a57bce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Get:1 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n",
            "Hit:2 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:3 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,632 B]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Get:5 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Get:6 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [1,378 kB]\n",
            "Get:7 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Get:9 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease [18.1 kB]\n",
            "Get:10 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ Packages [69.9 kB]\n",
            "Get:11 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease [24.3 kB]\n",
            "Hit:12 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:13 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,678 kB]\n",
            "Get:14 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [3,000 kB]\n",
            "Get:15 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [8,757 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,535 kB]\n",
            "Get:17 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,236 kB]\n",
            "Get:18 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy/main amd64 Packages [33.6 kB]\n",
            "Get:19 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [2,692 kB]\n",
            "Get:20 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy/main amd64 Packages [46.8 kB]\n",
            "Fetched 21.9 MB in 5s (4,773 kB/s)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "37 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "\u001b[1;33mW: \u001b[0mSkipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\u001b[0m\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "ffmpeg is already the newest version (7:4.4.2-0ubuntu0.22.04.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 37 not upgraded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import whisper\n",
        "\n",
        "model = whisper.load_model(\"medium\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_iL9MJz1T8n7",
        "outputId": "6d9f9966-76c7-42b3-9e14-45a971f4f018"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|█████████████████████████████████████| 1.42G/1.42G [00:22<00:00, 67.9MiB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchinfo"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dWd2dSjRP6Gg",
        "outputId": "0aceced9-383e-41f9-8fc1-e38145a44c16"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchinfo\n",
            "  Downloading torchinfo-1.8.0-py3-none-any.whl.metadata (21 kB)\n",
            "Downloading torchinfo-1.8.0-py3-none-any.whl (23 kB)\n",
            "Installing collected packages: torchinfo\n",
            "Successfully installed torchinfo-1.8.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchinfo import summary\n",
        "\n",
        "summary(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UNdAJ-RnQCq7",
        "outputId": "1b5e828e-c3e1-4cc3-e0fe-66f942b7a4fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "===========================================================================\n",
              "Layer (type:depth-idx)                             Param #\n",
              "===========================================================================\n",
              "Whisper                                            --\n",
              "├─AudioEncoder: 1-1                                --\n",
              "│    └─Conv1d: 2-1                                 246,784\n",
              "│    └─Conv1d: 2-2                                 3,146,752\n",
              "│    └─ModuleList: 2-3                             --\n",
              "│    │    └─ResidualAttentionBlock: 3-1            12,595,200\n",
              "│    │    └─ResidualAttentionBlock: 3-2            12,595,200\n",
              "│    │    └─ResidualAttentionBlock: 3-3            12,595,200\n",
              "│    │    └─ResidualAttentionBlock: 3-4            12,595,200\n",
              "│    │    └─ResidualAttentionBlock: 3-5            12,595,200\n",
              "│    │    └─ResidualAttentionBlock: 3-6            12,595,200\n",
              "│    │    └─ResidualAttentionBlock: 3-7            12,595,200\n",
              "│    │    └─ResidualAttentionBlock: 3-8            12,595,200\n",
              "│    │    └─ResidualAttentionBlock: 3-9            12,595,200\n",
              "│    │    └─ResidualAttentionBlock: 3-10           12,595,200\n",
              "│    │    └─ResidualAttentionBlock: 3-11           12,595,200\n",
              "│    │    └─ResidualAttentionBlock: 3-12           12,595,200\n",
              "│    │    └─ResidualAttentionBlock: 3-13           12,595,200\n",
              "│    │    └─ResidualAttentionBlock: 3-14           12,595,200\n",
              "│    │    └─ResidualAttentionBlock: 3-15           12,595,200\n",
              "│    │    └─ResidualAttentionBlock: 3-16           12,595,200\n",
              "│    │    └─ResidualAttentionBlock: 3-17           12,595,200\n",
              "│    │    └─ResidualAttentionBlock: 3-18           12,595,200\n",
              "│    │    └─ResidualAttentionBlock: 3-19           12,595,200\n",
              "│    │    └─ResidualAttentionBlock: 3-20           12,595,200\n",
              "│    │    └─ResidualAttentionBlock: 3-21           12,595,200\n",
              "│    │    └─ResidualAttentionBlock: 3-22           12,595,200\n",
              "│    │    └─ResidualAttentionBlock: 3-23           12,595,200\n",
              "│    │    └─ResidualAttentionBlock: 3-24           12,595,200\n",
              "│    └─LayerNorm: 2-4                              2,048\n",
              "├─TextDecoder: 1-2                                 458,752\n",
              "│    └─Embedding: 2-5                              53,109,760\n",
              "│    └─ModuleList: 2-6                             --\n",
              "│    │    └─ResidualAttentionBlock: 3-25           16,794,624\n",
              "│    │    └─ResidualAttentionBlock: 3-26           16,794,624\n",
              "│    │    └─ResidualAttentionBlock: 3-27           16,794,624\n",
              "│    │    └─ResidualAttentionBlock: 3-28           16,794,624\n",
              "│    │    └─ResidualAttentionBlock: 3-29           16,794,624\n",
              "│    │    └─ResidualAttentionBlock: 3-30           16,794,624\n",
              "│    │    └─ResidualAttentionBlock: 3-31           16,794,624\n",
              "│    │    └─ResidualAttentionBlock: 3-32           16,794,624\n",
              "│    │    └─ResidualAttentionBlock: 3-33           16,794,624\n",
              "│    │    └─ResidualAttentionBlock: 3-34           16,794,624\n",
              "│    │    └─ResidualAttentionBlock: 3-35           16,794,624\n",
              "│    │    └─ResidualAttentionBlock: 3-36           16,794,624\n",
              "│    │    └─ResidualAttentionBlock: 3-37           16,794,624\n",
              "│    │    └─ResidualAttentionBlock: 3-38           16,794,624\n",
              "│    │    └─ResidualAttentionBlock: 3-39           16,794,624\n",
              "│    │    └─ResidualAttentionBlock: 3-40           16,794,624\n",
              "│    │    └─ResidualAttentionBlock: 3-41           16,794,624\n",
              "│    │    └─ResidualAttentionBlock: 3-42           16,794,624\n",
              "│    │    └─ResidualAttentionBlock: 3-43           16,794,624\n",
              "│    │    └─ResidualAttentionBlock: 3-44           16,794,624\n",
              "│    │    └─ResidualAttentionBlock: 3-45           16,794,624\n",
              "│    │    └─ResidualAttentionBlock: 3-46           16,794,624\n",
              "│    │    └─ResidualAttentionBlock: 3-47           16,794,624\n",
              "│    │    └─ResidualAttentionBlock: 3-48           16,794,624\n",
              "│    └─LayerNorm: 2-7                              2,048\n",
              "===========================================================================\n",
              "Total params: 762,321,920\n",
              "Trainable params: 762,321,920\n",
              "Non-trainable params: 0\n",
              "==========================================================================="
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf dataset"
      ],
      "metadata": {
        "id": "WYzk4etxNxY5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "\"\"\"\n",
        "# get current working directory\n",
        "cwd = os.getcwd()\n",
        "\n",
        "# Build the full file path\n",
        "file_path = os.path.join(\"dataset\", \"audio\", \"1.wav\")\n",
        "\n",
        "# use the file path to transcribe the file\n",
        "result = model.transcribe(file_path)\n",
        "\n",
        "result[\"text\"]\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "UTHHUPBvUsh6",
        "outputId": "f0b220de-f447-40a0-bb7f-6c35e85845f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n# get current working directory\\ncwd = os.getcwd()\\n\\n# Build the full file path\\nfile_path = os.path.join(\"dataset\", \"audio\", \"1.wav\")\\n\\n# use the file path to transcribe the file\\nresult = model.transcribe(file_path)\\n\\nresult[\"text\"]\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " Early pregnancy LMP 4th October 2024 EDD 11th July 2025 D 5 weeks use stage single interveterian gestational sac is seen Fetal pole is not seen yet Yawksak seen Decidual reaction present No membrane separation is seen MHD 1.45 mm 1.45 cm Maturity 5 weeks 5 days Right over a normal Left over a not seen History of removal noted"
      ],
      "metadata": {
        "id": "vbYmUJgpVvbY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " Early pregnancy LMP 4th October 2024 EDD 11th July 2025 D5 week 6 days Single intrauterine gestational sac is seen Fetal pole is not seen yet YAC sac seen Decidual reaction present No membrane separation is seen MHD 1.45 mm 1.45 cm Maturity 5 weeks 5 days Right over in normal Left over in odd scene History of removal noted"
      ],
      "metadata": {
        "id": "-dB1RQQaVf7I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Unzip dataset"
      ],
      "metadata": {
        "id": "7Ncu2NHpaiVi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# unzip dataset\n",
        "!unzip dataset.zip\n"
      ],
      "metadata": {
        "id": "i3lpBUsVVeTB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "66d746fa-92dd-46b3-9fd7-c22579e31e14"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  dataset.zip\n",
            "   creating: dataset/\n",
            "  inflating: dataset/1.jpg           \n",
            "  inflating: dataset/1.jpg.txt       \n",
            "  inflating: dataset/1.wav           \n",
            "  inflating: dataset/1.wav.txt       \n",
            "  inflating: dataset/2.jpg           \n",
            "  inflating: dataset/2.jpg.txt       \n",
            "  inflating: dataset/2.wav           \n",
            "  inflating: dataset/2.wav.txt       \n",
            "  inflating: dataset/3.jpg           \n",
            "  inflating: dataset/3.jpg.txt       \n",
            "  inflating: dataset/3.wav           \n",
            "  inflating: dataset/3.wav.txt       \n",
            "  inflating: dataset/4.jpg           \n",
            "  inflating: dataset/4.jpg.txt       \n",
            "  inflating: dataset/4.wav           \n",
            "  inflating: dataset/4.wav.txt       \n",
            "  inflating: dataset/5.jpg           \n",
            "  inflating: dataset/5.jpg.txt       \n",
            "  inflating: dataset/5.wav           \n",
            "  inflating: dataset/5.wav.txt       \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## move all the audio under one dir\n",
        "import shutil\n",
        "import pathlib\n",
        "from pathlib import Path\n",
        "\n",
        "audio_files = Path(\"dataset\") / \"audio\"\n",
        "audio_files.mkdir(parents=True, exist_ok=True) # Create audio directory if it doesn't exist\n",
        "\n",
        "for file_path in Path(\"dataset\").glob(\"*.wav\"):\n",
        "    shutil.move(str(file_path), str(audio_files))"
      ],
      "metadata": {
        "id": "7Ymgel-jY0Iy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# move all the transcribed text under one dir\n",
        "\n",
        "text = Path(\"dataset\") / \"text\"\n",
        "text.mkdir(parents=True, exist_ok=True) # Create audio directory if it doesn't exist\n",
        "\n",
        "for file_path in Path(\"dataset\").glob(\"*.wav.txt\"):\n",
        "    shutil.move(str(file_path), str(text))"
      ],
      "metadata": {
        "id": "zWFxKVzPcG44"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# move all the transcribed photos under one dir\n",
        "\n",
        "photo_txt = Path(\"dataset\") / \"photo_txt\"\n",
        "photo_txt.mkdir(parents=True, exist_ok=True) # Create audio directory if it doesn't exist\n",
        "\n",
        "for file_path in Path(\"dataset\").glob(\"*.jpg.txt\"):\n",
        "    shutil.move(str(file_path), str(photo_txt))"
      ],
      "metadata": {
        "id": "ssXx1LjzcqLO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# move all the transcribed photos under one dir\n",
        "\n",
        "photos = Path(\"dataset\") / \"photos\"\n",
        "photos.mkdir(parents=True, exist_ok=True) # Create audio directory if it doesn't exist\n",
        "\n",
        "for file_path in Path(\"dataset\").glob(\"*.jpg\"):\n",
        "    shutil.move(str(file_path), str(photos))"
      ],
      "metadata": {
        "id": "cC-UlxQkc5ss"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "files.download(\"dataset\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "txug7P7Uq3qO",
        "outputId": "a5a39019-dca2-49a5-ddb5-6bfda5591291"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_3a78df32-ff7c-4844-82ad-e315b6171a1f\", \"dataset\", 4096)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "audio_files, text"
      ],
      "metadata": {
        "id": "oho2iR_WdGOF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0be182b5-1a0f-48e2-f29f-f0144ffadd91"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(PosixPath('dataset/audio'), PosixPath('dataset/text'))"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Setting Up Some Functions"
      ],
      "metadata": {
        "id": "EAsmRPR0n-VL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import whisper\n",
        "from pathlib import Path\n",
        "\n",
        "def transcribe_audio_files(audio_dir, model):\n",
        "    \"\"\"\n",
        "    Transcribes all WAV audio files in a directory using the Whisper model.\n",
        "\n",
        "    Args:\n",
        "        audio_dir (str or Path): The path to the directory containing the audio files.\n",
        "        model (whisper.Whisper): The loaded Whisper model instance.\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary where keys are audio filenames and values are lists\n",
        "              containing the transcribed text for each file.\n",
        "    \"\"\"\n",
        "\n",
        "    transcribed_text = {}\n",
        "    audio_files = Path(audio_dir)\n",
        "\n",
        "    for filename in os.listdir(audio_files):\n",
        "        if filename.endswith(\".wav\"):\n",
        "            file_path = audio_files / filename\n",
        "            result = model.transcribe(str(file_path))\n",
        "            print(f\"Transcription for {filename}: {result['text']}\")\n",
        "            transcribed_text.setdefault(f\"{filename}\", []).append(result['text'])\n",
        "\n",
        "    return transcribed_text\n",
        "\n",
        "# Example usage:\n",
        "model = whisper.load_model(\"medium\")  # Assuming you have loaded the model\n",
        "audio_directory = \"dataset/audio\"  # Replace with your audio directory\n",
        "transcriptions = transcribe_audio_files(audio_directory, model)\n",
        "print(transcriptions)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7-_kFl5HdHgn",
        "outputId": "f787f644-e8c7-4385-da6f-eb7aa8a7922d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transcription for 3.wav:  Single live intravertebrate and fitters is seen in vertex presentation. Fetal cardiac activity and movements are appreciated. FHR 136 per minute, lycoris lace, AFI 8.6 cm, placenta is posterior left lateral, 3 vessel cord is seen, BPD 8.66, HC 32.7, AC 30.60, FL 6.62, HL 6.07, average 35 plus 2, weight 2469, colour Doppler examination within normal limits, mild oligohydranias.\n",
            "Transcription for 2.wav:  staggering Eresch בה empowering Baidaya\n",
            "Transcription for 5.wav:  Early obstetrics sonography LMP 15th November 2024, EDD 22nd August 2025, D 7 weeks, Gravidulatory shows smooth wall gestational sac, Fetal pole is seen, cardiac activity is appreciated, decidual reaction good, CRL 3 millimeters, 5 weeks 6 days, tiny blood collection is seen just above internal loss measuring 5.9 into 2.8 millimeter, cerebics 4 centimeters, no adenoxal pathology, no free fluidity in pouch of Douglas, delayed conception UHGDD 30 August 2025.\n",
            "Transcription for 1.wav:  Early pregnancy LMP 4th October 2024, EDD 11th July 2025, D 5 week 6 days, single intra uterine gestational sac is seen, fetal pole is not seen yet, yolk sac seen, decidual reaction present, no membrane separation is seen, MLD 1.45 millimeter, 1.45 centimeters, maturity 5 weeks 5 days, right ovary normal, left ovary not seen, history of removal noted.\n",
            "Transcription for 4.wav:  Velvic ultrasound examination, uterus is bulky globular and shows adenomyotic changes, measures 8.5 into 6.34 into 5.7 centimeters, endometrial echo is central and normal, ET 6.6 both over is normal, no adenoxal pathology, no free fluid is seen in pouch of Douglas, adenomyosis uterus.\n",
            "{'3.wav': [' Single live intravertebrate and fitters is seen in vertex presentation. Fetal cardiac activity and movements are appreciated. FHR 136 per minute, lycoris lace, AFI 8.6 cm, placenta is posterior left lateral, 3 vessel cord is seen, BPD 8.66, HC 32.7, AC 30.60, FL 6.62, HL 6.07, average 35 plus 2, weight 2469, colour Doppler examination within normal limits, mild oligohydranias.'], '2.wav': [' staggering Eresch בה empowering Baidaya'], '5.wav': [' Early obstetrics sonography LMP 15th November 2024, EDD 22nd August 2025, D 7 weeks, Gravidulatory shows smooth wall gestational sac, Fetal pole is seen, cardiac activity is appreciated, decidual reaction good, CRL 3 millimeters, 5 weeks 6 days, tiny blood collection is seen just above internal loss measuring 5.9 into 2.8 millimeter, cerebics 4 centimeters, no adenoxal pathology, no free fluidity in pouch of Douglas, delayed conception UHGDD 30 August 2025.'], '1.wav': [' Early pregnancy LMP 4th October 2024, EDD 11th July 2025, D 5 week 6 days, single intra uterine gestational sac is seen, fetal pole is not seen yet, yolk sac seen, decidual reaction present, no membrane separation is seen, MLD 1.45 millimeter, 1.45 centimeters, maturity 5 weeks 5 days, right ovary normal, left ovary not seen, history of removal noted.'], '4.wav': [' Velvic ultrasound examination, uterus is bulky globular and shows adenomyotic changes, measures 8.5 into 6.34 into 5.7 centimeters, endometrial echo is central and normal, ET 6.6 both over is normal, no adenoxal pathology, no free fluid is seen in pouch of Douglas, adenomyosis uterus.']}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "transcriptions"
      ],
      "metadata": {
        "id": "-MfuqRVCfbiD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca079855-170e-4f0e-875b-6224cf3b87ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'3.wav': [' Single live intravertebrate and fitters is seen in vertex presentation. Fetal cardiac activity and movements are appreciated. FHR 136 per minute, lycoris lace, AFI 8.6 cm, placenta is posterior left lateral, 3 vessel cord is seen, BPD 8.66, HC 32.7, AC 30.60, FL 6.62, HL 6.07, average 35 plus 2, weight 2469, colour Doppler examination within normal limits, mild oligohydranias.'],\n",
              " '2.wav': [' staggering Eresch בה empowering Baidaya'],\n",
              " '5.wav': [' Early obstetrics sonography LMP 15th November 2024, EDD 22nd August 2025, D 7 weeks, Gravidulatory shows smooth wall gestational sac, Fetal pole is seen, cardiac activity is appreciated, decidual reaction good, CRL 3 millimeters, 5 weeks 6 days, tiny blood collection is seen just above internal loss measuring 5.9 into 2.8 millimeter, cerebics 4 centimeters, no adenoxal pathology, no free fluidity in pouch of Douglas, delayed conception UHGDD 30 August 2025.'],\n",
              " '1.wav': [' Early pregnancy LMP 4th October 2024, EDD 11th July 2025, D 5 week 6 days, single intra uterine gestational sac is seen, fetal pole is not seen yet, yolk sac seen, decidual reaction present, no membrane separation is seen, MLD 1.45 millimeter, 1.45 centimeters, maturity 5 weeks 5 days, right ovary normal, left ovary not seen, history of removal noted.'],\n",
              " '4.wav': [' Velvic ultrasound examination, uterus is bulky globular and shows adenomyotic changes, measures 8.5 into 6.34 into 5.7 centimeters, endometrial echo is central and normal, ET 6.6 both over is normal, no adenoxal pathology, no free fluid is seen in pouch of Douglas, adenomyosis uterus.']}"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install jiwer"
      ],
      "metadata": {
        "id": "UU-wIrnMeZqh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "123928be-6f07-4690-c700-56c85bb655eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting jiwer\n",
            "  Downloading jiwer-3.1.0-py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: click>=8.1.8 in /usr/local/lib/python3.11/dist-packages (from jiwer) (8.1.8)\n",
            "Collecting rapidfuzz>=3.9.7 (from jiwer)\n",
            "  Downloading rapidfuzz-3.12.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Downloading jiwer-3.1.0-py3-none-any.whl (22 kB)\n",
            "Downloading rapidfuzz-3.12.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m48.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: rapidfuzz, jiwer\n",
            "Successfully installed jiwer-3.1.0 rapidfuzz-3.12.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load Whisper-medium"
      ],
      "metadata": {
        "id": "O4lhmh4SpZAN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "whisper_medium = whisper.load_model(\"medium\")"
      ],
      "metadata": {
        "id": "NnndJGZeoxI9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# calculate WER\n",
        "import jiwer\n",
        "\n",
        "def calculate_wer(reference, hypothesis):\n",
        "    \"\"\"Calculates Word Error Rate (WER).\"\"\"\n",
        "\n",
        "    wer = jiwer.wer(reference, hypothesis)\n",
        "    return wer\n",
        "\n",
        "# Example usage\n",
        "reference = \"This is the reference text.\"\n",
        "hypothesis = \"This is the transcribed text.\"\n",
        "\n",
        "wer = calculate_wer(reference, hypothesis)\n",
        "print(f\"WER: {wer}\")"
      ],
      "metadata": {
        "id": "yMONU2MIdzXl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5c9c323a-6b42-426e-b9c3-53b1f5f39b45"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WER: 0.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import jiwer\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "def calculate_wer_from_dirs(transcribed_text, textdir):\n",
        "    \"\"\"\n",
        "    Calculates the average Word Error Rate (WER) for a set of audio files\n",
        "    and their corresponding reference transcripts.\n",
        "\n",
        "    Args:\n",
        "        transcribed_text (dict): A dictionary where keys are audio filenames\n",
        "                                  (e.g., '1.wav') and values are lists\n",
        "                                  containing the transcribed text.\n",
        "        textdir (str or Path): The path to the directory containing the\n",
        "                                reference transcript files\n",
        "                                (e.g., 'dataset/text').\n",
        "\n",
        "    Returns:\n",
        "        float: The average WER across all audio files.\n",
        "    \"\"\"\n",
        "\n",
        "    total_wer = 0\n",
        "    num_files = 0\n",
        "\n",
        "    for audio_filename, hypothesis in transcribed_text.items():\n",
        "        # Construct the expected reference file name\n",
        "        reference_filename = audio_filename[:-4] + \".wav.txt\"  # Remove '.wav' and add '.wav.txt'\n",
        "        reference_filepath = Path(textdir) / reference_filename\n",
        "\n",
        "        # Read the reference text from the file\n",
        "        with open(reference_filepath, 'r') as f:\n",
        "            reference = f.read().strip()\n",
        "\n",
        "        # Calculate WER for this file\n",
        "        wer = jiwer.wer(reference, hypothesis[0])  # hypothesis is a list, take the first element\n",
        "        total_wer += wer\n",
        "        num_files += 1\n",
        "\n",
        "    # Calculate and return the average WER\n",
        "    average_wer = total_wer / num_files if num_files else 0\n",
        "    return average_wer\n",
        "\n",
        "# Example usage:\n",
        "textdir = \"dataset/text\"  # Path to your reference transcripts\n",
        "average_wer = calculate_wer_from_dirs(transcriptions, textdir)\n",
        "print(f\"Average WER: {average_wer}\")"
      ],
      "metadata": {
        "id": "CNmccdf8f7J7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "99078bd9-adbf-4996-ac0c-62abad8579a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average WER: 0.5697828172364836\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import jiwer\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "def calculate_asr_metrics(transcribed_text, textdir):\n",
        "    \"\"\"\n",
        "    Calculates Word Error Rate (WER), Match Error Rate (MER),\n",
        "    Word Information Lost (WIL), Word Information Preserved (WIP),\n",
        "    and Character Error Rate (CER) for a set of audio files and their\n",
        "    corresponding reference transcripts.\n",
        "\n",
        "    Args:\n",
        "        transcribed_text (dict): A dictionary where keys are audio filenames\n",
        "                                  (e.g., '1.wav') and values are lists\n",
        "                                  containing the transcribed text.\n",
        "        textdir (str or Path): The path to the directory containing the\n",
        "                                reference transcript files\n",
        "                                (e.g., 'dataset/text').\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary containing the calculated metrics (WER, MER, WIL, WIP, CER).\n",
        "    \"\"\"\n",
        "\n",
        "    metrics = {\n",
        "        \"WER\": 0,\n",
        "        \"MER\": 0,\n",
        "        \"WIL\": 0,\n",
        "        \"WIP\": 0,\n",
        "        \"CER\": 0\n",
        "    }\n",
        "    num_files = 0\n",
        "\n",
        "    for audio_filename, hypothesis in transcribed_text.items():\n",
        "        # Construct the expected reference file name\n",
        "        reference_filename = audio_filename[:-4] + \".wav.txt\"\n",
        "        reference_filepath = Path(textdir) / reference_filename\n",
        "\n",
        "        # Read the reference text from the file\n",
        "        with open(reference_filepath, 'r') as f:\n",
        "            reference = f.read().strip()\n",
        "\n",
        "        # Calculate metrics for this file\n",
        "        metrics[\"WER\"] += jiwer.wer(reference, hypothesis[0])\n",
        "        metrics[\"MER\"] += jiwer.mer(reference, hypothesis[0])\n",
        "        metrics[\"WIL\"] += jiwer.wil(reference, hypothesis[0])\n",
        "        metrics[\"WIP\"] += jiwer.wip(reference, hypothesis[0])\n",
        "        metrics[\"CER\"] += jiwer.cer(reference, hypothesis[0])\n",
        "\n",
        "        num_files += 1\n",
        "\n",
        "    # Calculate average metrics\n",
        "    for metric in metrics:\n",
        "        metrics[metric] /= num_files if num_files else 1  # Avoid division by zero\n",
        "\n",
        "    return metrics\n",
        "\n",
        "# Example usage:\n",
        "textdir = \"dataset/text\"  # Path to your reference transcripts\n",
        "results = calculate_asr_metrics(transcriptions, textdir)\n",
        "print(f\"Metrics: {results}\")"
      ],
      "metadata": {
        "id": "iQjSOT9Jge4C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "efae1c6e-afcc-4083-dafb-3aa21522b48f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Metrics: {'WER': 0.5697828172364836, 'MER': 0.5607119049240896, 'WIL': 0.7371895148718928, 'WIP': 0.2628104851281072, 'CER': 0.31947388650037956}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "transcriptions_medium = transcribe_audio_files(audio_directory, model)\n",
        "print(transcriptions_medium)"
      ],
      "metadata": {
        "id": "tv5_JlkphPWV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5eef9127-6043-489e-c02c-d4d6d94f93a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transcription for 3.wav:  Single live intravertebrate and fitters is seen in vertex presentation. Fetal cardiac activity and movements are appreciated. FHR 136 per minute, lycoris lace, AFI 8.6 cm, placenta is posterior left lateral, 3 vessel cord is seen, BPD 8.66, HC 32.7, AC 30.60, FL 6.62, HL 6.07, average 35 plus 2, weight 2469, colour Doppler examination within normal limits, mild oligohydranias.\n",
            "Transcription for 2.wav:   jesteśmy apke ‌ alive ‌ اض Sithim  Алекс ‌ alors বেবববববববববববববববববববববববববববববববববববববববববববববববববববববববববববববববববববববববববববববববববববববববববববববববববববববববববববব�\n",
            "Transcription for 5.wav:  Early obstetrics sonography LMP 15th November 2024, EDD 22nd August 2025, D 7 weeks, Gravidulatory shows smooth wall gestational sac, Fetal pole is seen, cardiac activity is appreciated, decidual reaction good, CRL 3 millimeters, 5 weeks 6 days, tiny blood collection is seen just above internal loss measuring 5.9 into 2.8 millimeter, cerebics 4 centimeters, no adenoxal pathology, no free fluidity in pouch of Douglas, delayed conception UHGDD 30 August 2025.\n",
            "Transcription for 1.wav:  Early pregnancy LMP 4th October 2024, EDD 11th July 2025, D 5 week 6 days, single intra uterine gestational sac is seen, fetal pole is not seen yet, yolk sac seen, decidual reaction present, no membrane separation is seen, MLD 1.45 millimeter, 1.45 centimeters, maturity 5 weeks 5 days, right ovary normal, left ovary not seen, history of removal noted.\n",
            "Transcription for 4.wav:  Velvic ultrasound examination, uterus is bulky globular and shows adenomyotic changes, measures 8.5 into 6.34 into 5.7 centimeters, endometrial echo is central and normal, ET 6.6 both over is normal, no adenoxal pathology, no free fluid is seen in pouch of Douglas, adenomyosis uterus.\n",
            "{'3.wav': [' Single live intravertebrate and fitters is seen in vertex presentation. Fetal cardiac activity and movements are appreciated. FHR 136 per minute, lycoris lace, AFI 8.6 cm, placenta is posterior left lateral, 3 vessel cord is seen, BPD 8.66, HC 32.7, AC 30.60, FL 6.62, HL 6.07, average 35 plus 2, weight 2469, colour Doppler examination within normal limits, mild oligohydranias.'], '2.wav': ['  jesteśmy apke \\u200c alive \\u200c اض Sithim  Алекс \\u200c alors বেবববববববববববববববববববববববববববববববববববববববববববববববববববববববববববববববববববববববববববববববববববববববববববববববববববববববববববব�'], '5.wav': [' Early obstetrics sonography LMP 15th November 2024, EDD 22nd August 2025, D 7 weeks, Gravidulatory shows smooth wall gestational sac, Fetal pole is seen, cardiac activity is appreciated, decidual reaction good, CRL 3 millimeters, 5 weeks 6 days, tiny blood collection is seen just above internal loss measuring 5.9 into 2.8 millimeter, cerebics 4 centimeters, no adenoxal pathology, no free fluidity in pouch of Douglas, delayed conception UHGDD 30 August 2025.'], '1.wav': [' Early pregnancy LMP 4th October 2024, EDD 11th July 2025, D 5 week 6 days, single intra uterine gestational sac is seen, fetal pole is not seen yet, yolk sac seen, decidual reaction present, no membrane separation is seen, MLD 1.45 millimeter, 1.45 centimeters, maturity 5 weeks 5 days, right ovary normal, left ovary not seen, history of removal noted.'], '4.wav': [' Velvic ultrasound examination, uterus is bulky globular and shows adenomyotic changes, measures 8.5 into 6.34 into 5.7 centimeters, endometrial echo is central and normal, ET 6.6 both over is normal, no adenoxal pathology, no free fluid is seen in pouch of Douglas, adenomyosis uterus.']}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results = calculate_asr_metrics(transcriptions_medium, textdir)\n",
        "print(f\"Metrics: {results}\")"
      ],
      "metadata": {
        "id": "h5QB3y7Qp7x2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9296f520-74e4-4492-9ae1-5730acdff4a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Metrics: {'WER': 0.5697828172364836, 'MER': 0.5607119049240896, 'WIL': 0.7371895148718928, 'WIP': 0.2628104851281072, 'CER': 0.3182316504755348}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "RUNNING SEVERAL EXPERIMENTS"
      ],
      "metadata": {
        "id": "7YdO21E5r0kR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install jiwer tensorflow"
      ],
      "metadata": {
        "id": "4PxTqwKT3_M3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai-whisper"
      ],
      "metadata": {
        "id": "JAhEt7lcr5rR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "82826ab5-6810-49b3-f991-ba491127274e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai-whisper\n",
            "  Downloading openai-whisper-20240930.tar.gz (800 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/800.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m716.8/800.5 kB\u001b[0m \u001b[31m21.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m800.5/800.5 kB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (0.60.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (2.0.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (2.6.0+cu124)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (4.67.1)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (10.6.0)\n",
            "Collecting tiktoken (from openai-whisper)\n",
            "  Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: triton>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (3.2.0)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba->openai-whisper) (0.43.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken->openai-whisper) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken->openai-whisper) (2.32.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (2024.10.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch->openai-whisper)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch->openai-whisper)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch->openai-whisper)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch->openai-whisper)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch->openai-whisper)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch->openai-whisper)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch->openai-whisper)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch->openai-whisper)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch->openai-whisper)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch->openai-whisper)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->openai-whisper) (1.3.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (2025.1.31)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->openai-whisper) (3.0.2)\n",
            "Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m54.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m109.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m90.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m55.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m85.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: openai-whisper\n",
            "  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for openai-whisper: filename=openai_whisper-20240930-py3-none-any.whl size=803375 sha256=640aa115896a048eafaf376f49550a5099079965fe3bb11c8a8d1790e5185bec\n",
            "  Stored in directory: /root/.cache/pip/wheels/2f/f2/ce/6eb23db4091d026238ce76703bd66da60b969d70bcc81d5d3a\n",
            "Successfully built openai-whisper\n",
            "Installing collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, tiktoken, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, openai-whisper\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 openai-whisper-20240930 tiktoken-0.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import whisper\n",
        "import jiwer\n",
        "import os\n",
        "from pathlib import Path\n",
        "import tensorflow as tf\n",
        "from datetime import datetime\n",
        "import time\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def run_and_test_models(audio_dir, text_dir, models):\n",
        "    \"\"\"\n",
        "    Runs and tests multiple audio-to-text models,\n",
        "    logs metrics to TensorBoard, and visualizes results.\n",
        "\n",
        "    Args:\n",
        "        audio_dir (str or Path): The path to the directory containing the audio files.\n",
        "        text_dir (str or Path): The path to the directory containing the reference transcripts.\n",
        "        models (list): A list of model names or paths to load.\n",
        "                        For Whisper models, you can use 'tiny', 'base', 'small', 'medium', 'large'.\n",
        "                        For other models, you might need to specify their paths or loading functions.\n",
        "\n",
        "    Returns:\n",
        "        None\n",
        "    \"\"\"\n",
        "\n",
        "    # Create a TensorBoard log directory\n",
        "    log_dir = \"logs/audio_to_text/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "    summary_writer = tf.summary.create_file_writer(log_dir)\n",
        "\n",
        "    # Store results for visualization\n",
        "    results_data = []\n",
        "\n",
        "    for model_name in models:\n",
        "        print(f\"Processing model: {model_name}\")\n",
        "\n",
        "        # Load the model\n",
        "        if model_name in [\"tiny\", \"base\", \"small\", \"medium\", \"large\"]:\n",
        "            model = whisper.load_model(model_name)\n",
        "        else:\n",
        "            # Handle loading other models here (e.g., using specific libraries)\n",
        "            # Replace with your own loading logic\n",
        "            raise NotImplementedError(f\"Model loading for {model_name} not implemented.\")\n",
        "\n",
        "        # Get model size (Updated)\n",
        "        # Get the download root directory for Whisper models\n",
        "        # Instead of accessing the `_download_root` attribute, directly construct\n",
        "        # the model file path within the Whisper cache directory\n",
        "        download_root = os.path.join(os.path.expanduser(\"~\"), \".cache\", \"whisper\")\n",
        "\n",
        "        # Construct the expected model file path\n",
        "        model_dir = os.path.join(download_root, model_name + \".pt\")\n",
        "\n",
        "        # Check if the model file exists before getting its size\n",
        "        if os.path.exists(model_dir):\n",
        "            model_size = os.path.getsize(model_dir) / (1024 * 1024)  # Calculate model size in MB\n",
        "        else:\n",
        "            model_size = -1  # or some other indicator that the file wasn't found\n",
        "            print(f\"Warning: Model file not found at {model_dir}. Size will be reported as -1.\")\n",
        "\n",
        "        # Transcribe audio files and measure time\n",
        "        start_time = time.time()\n",
        "        transcriptions = transcribe_audio_files(audio_dir, model)\n",
        "        transcription_time = time.time() - start_time\n",
        "\n",
        "        # Calculate metrics\n",
        "        metrics = calculate_asr_metrics(transcriptions, text_dir)\n",
        "\n",
        "        # Store results\n",
        "        # Store results\n",
        "        results_data.append({\n",
        "            \"model\": model_name,\n",
        "            \"transcription_time (s)\": transcription_time,  # Include transcription time\n",
        "            **metrics  # Add metrics to the dictionary\n",
        "        })\n",
        "        \"\"\"\n",
        "        # Log metrics to TensorBoard\n",
        "        with summary_writer.as_default():\n",
        "            for metric_name, metric_value in metrics.items():\n",
        "                tf.summary.scalar(f\"{model_name}/{metric_name}\", metric_value, step=1)\n",
        "\n",
        "    print(f\"TensorBoard logs saved to: {log_dir}\")\n",
        "         \"\"\"\n",
        "    return results_data\n"
      ],
      "metadata": {
        "id": "ZH8ltiA8r_l4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "models = [\"tiny\", \"base\", \"medium\"] ## whisper\n",
        "\n",
        "audio_directory = \"dataset/audio\"\n",
        "text_directory = \"dataset/text\"\n",
        "\n",
        "results_data = run_and_test_models(audio_directory, text_directory, models)"
      ],
      "metadata": {
        "id": "kKdrD_VlsGVB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c0d0b071-473f-4add-c657-98645765b44a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing model: tiny\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████████████████████████████████| 72.1M/72.1M [00:00<00:00, 109MiB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing model: base\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|████████████████████████████████████████| 139M/139M [00:01<00:00, 137MiB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing model: medium\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualization using Pandas and Matplotlib\n",
        "results_df = pd.DataFrame(results_data)\n",
        "results_df.set_index(\"model\", inplace=True)\n",
        "\n",
        "# Select only the metrics columns (excluding transcription time)\n",
        "metrics_to_plot = [col for col in results_df.columns if col != \"transcription_time (s)\"]\n",
        "metrics_df = results_df[metrics_to_plot]\n",
        "\n",
        "# Transpose for plotting\n",
        "metrics_df = metrics_df.T\n",
        "\n",
        "# Plotting all metrics in one plot\n",
        "ax = metrics_df.plot(kind=\"bar\", figsize=(12, 8), colormap=\"viridis\")\n",
        "plt.title(\"ASR Metrics by Model\")\n",
        "plt.ylabel(\"Metric Value\")\n",
        "plt.xlabel(\"Metric\")\n",
        "plt.xticks(rotation=45, ha=\"right\")\n",
        "plt.legend(title=\"Model\")\n",
        "plt.tight_layout()\n",
        "\n",
        "# ... (Optional: add data labels) ...\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "OCighy94sJiX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 807
        },
        "outputId": "79e74524-469e-4e40-b16b-cbb76d2d2584"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x800 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#%load_ext tensorboard\n",
        "#%tensorboard --logdir logs/audio_to_text"
      ],
      "metadata": {
        "id": "0vIjwYAqxItU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# set up device agnostic code\n",
        "import torch\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ],
      "metadata": {
        "id": "uhDqmVVS7bY_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Project Demo"
      ],
      "metadata": {
        "id": "TNlppIaKVEg-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# make project dir\n",
        "from pathlib import Path\n",
        "\n",
        "project_dir = Path(\"WhisperASR\")\n",
        "project_dir.mkdir(parents = True, exist_ok = True)"
      ],
      "metadata": {
        "id": "CH55OnE9uXX2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load the model\n",
        "import whisper\n",
        "\n",
        "whisper_medium = whisper.load_model(\"medium\")"
      ],
      "metadata": {
        "id": "6N5jMlhetEMt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "627c8d12-f0af-409a-8447-0454cecd7340"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|█████████████████████████████████████| 1.42G/1.42G [00:26<00:00, 56.6MiB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#transcribe function\n",
        "import whisper\n",
        "from pathlib import Path\n",
        "\n",
        "def transcribe_audio_file(audio_file_path, model=\"medium\"):\n",
        "    \"\"\"\n",
        "    Transcribes a single WAV audio file using the Whisper model.\n",
        "\n",
        "    Args:\n",
        "        audio_file_path (str or Path): The path to the audio file.\n",
        "        model (str, optional): The name of the Whisper model to use\n",
        "                               (e.g., \"tiny\", \"base\", \"small\", \"medium\", \"large\").\n",
        "                               Defaults to \"medium\".\n",
        "\n",
        "    Returns:\n",
        "        str: The transcribed text from the audio file.\n",
        "    \"\"\"\n",
        "\n",
        "    # Load the Whisper model\n",
        "    model = whisper.load_model(model)\n",
        "\n",
        "    # Transcribe the audio file\n",
        "    result = model.transcribe(str(audio_file_path))\n",
        "\n",
        "    # Return the transcribed text\n",
        "    return result[\"text\"]\n",
        "\n",
        "# Example usage:\n",
        "audio_file = \"dataset/audio/1.wav\"\n",
        "transcribed_text = transcribe_audio_file(audio_file)\n",
        "print(f\"Transcription: {transcribed_text}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fN3Tg9FGVDbE",
        "outputId": "b15bb366-2bef-4142-8584-d56ea1e11a44"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transcription:  Early pregnancy LMP 4th October 2024, EDD 11th July 2025, D 5 week 6 days, single intra uterine gestational sac is seen, fetal pole is not seen yet, yolk sac seen, decidual reaction present, no membrane separation is seen, MLD 1.45 millimeter, 1.45 centimeters, maturity 5 weeks 5 days, right ovary normal, left ovary not seen, history of removal noted.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import torch\n",
        "import torch\n",
        "import os\n",
        "\n",
        "model_dir = project_dir / \"models\"\n",
        "model_dir.mkdir(parents = True, exist_ok = True)\n",
        "\n",
        "model_save_path = os.path.join(model_dir, \"whisper_medium.pth\")\n",
        "torch.save(obj = whisper_medium.state_dict(), f = model_save_path)"
      ],
      "metadata": {
        "id": "izWVuiS8WOh6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# save tiny and base too\n",
        "import whisper\n",
        "import os\n",
        "\n",
        "whisper_base = whisper.load_model(\"base\")\n",
        "whisper_tiny = whisper.load_model(\"tiny\")\n",
        "\n",
        "# save them\n",
        "model_save_path = os.path.join(model_dir, \"whisper_base.pth\")\n",
        "torch.save(obj = whisper_base.state_dict(), f = model_save_path)\n",
        "\n",
        "model_save_path = os.path.join(model_dir, \"whisper_tiny.pth\")\n",
        "torch.save(obj = whisper_tiny.state_dict(), f = model_save_path)"
      ],
      "metadata": {
        "id": "xDp96SDg2hSJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "85373f37-eb7d-4e54-d19c-a246cab57b6b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|███████████████████████████████████████| 139M/139M [00:02<00:00, 59.6MiB/s]\n",
            "100%|█████████████████████████████████████| 72.1M/72.1M [00:02<00:00, 27.9MiB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# check its stats\n",
        "#whisper_medium.load_state_dict(f = \"whisper_medium.pth\")\n",
        "Path(model_save_path).stat().st_size // (1024 * 1024)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0B9R1jYLWZZL",
        "outputId": "4ee4bab7-a448-4784-87ad-3e59e98759b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "144"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q gradio"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rzplEsDSYQr1",
        "outputId": "67049714-d6c7-432b-ab2c-621c554c5f63"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.2/46.2 MB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m322.2/322.2 kB\u001b[0m \u001b[31m27.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.9/94.9 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.3/11.3 MB\u001b[0m \u001b[31m114.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2nd Test For The Demo"
      ],
      "metadata": {
        "id": "SW7Oc0QQcoHB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pytesseract pillow"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MOLYDCNdYj8M",
        "outputId": "e96b8f44-606f-4f1a-e2ea-e9f161d5fe1e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pytesseract\n",
            "  Downloading pytesseract-0.3.13-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (11.1.0)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.11/dist-packages (from pytesseract) (24.2)\n",
            "Downloading pytesseract-0.3.13-py3-none-any.whl (14 kB)\n",
            "Installing collected packages: pytesseract\n",
            "Successfully installed pytesseract-0.3.13\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fpdf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "01CDf3UidL51",
        "outputId": "54f17cb1-3b59-4893-f1a3-eafaf2f2c3fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fpdf\n",
            "  Downloading fpdf-1.7.2.tar.gz (39 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: fpdf\n",
            "  Building wheel for fpdf (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fpdf: filename=fpdf-1.7.2-py2.py3-none-any.whl size=40704 sha256=1b0c1ffc8ee9e51c377e3d6d83c59d7d5989b4b96980e3f4b9de13960a16ea8d\n",
            "  Stored in directory: /root/.cache/pip/wheels/65/4f/66/bbda9866da446a72e206d6484cd97381cbc7859a7068541c36\n",
            "Successfully built fpdf\n",
            "Installing collected packages: fpdf\n",
            "Successfully installed fpdf-1.7.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "example_audios = project_dir / \"example_audios\"\n",
        "example_audios.mkdir(parents = True, exist_ok = True)"
      ],
      "metadata": {
        "id": "THbG5h5H5mVS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "\n",
        "source_dir = \"/content/dataset/audio\"\n",
        "destination_dir = \"/content/WhisperASR/example_audios\"\n",
        "\n",
        "files_to_copy = [\"1.wav\", \"4.wav\", \"5.wav\"]\n",
        "for filename in files_to_copy:\n",
        "    src_file = os.path.join(source_dir, filename)\n",
        "    dst_file = os.path.join(destination_dir, filename)\n",
        "\n",
        "    if os.path.exists(src_file):\n",
        "        shutil.copy(src_file, dst_file)\n",
        "        print(f\"✅ Copied {filename}\")\n",
        "    else:\n",
        "        print(f\"⚠️ {filename} not found at {src_file}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8FxkniZt9SbD",
        "outputId": "5202236a-78fe-4dd4-f034-2f0effcf687c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Copied 1.wav\n",
            "✅ Copied 4.wav\n",
            "✅ Copied 5.wav\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile WhisperASR/model.py\n",
        "import whisper\n",
        "import torch\n",
        "import os\n",
        "\n",
        "# ✅ Allowed Whisper models with custom local paths\n",
        "ALLOWED_MODELS = {\n",
        "    \"tiny\": \"models/whisper_tiny.pth\",\n",
        "    \"base\": \"models/whisper_base.pth\",\n",
        "    \"medium\": \"models/whisper_medium.pth\",\n",
        "}\n",
        "\n",
        "def load_whisper_model(model_name):\n",
        "    \"\"\"Load Whisper model from local weights.\"\"\"\n",
        "    if model_name not in ALLOWED_MODELS:\n",
        "        raise ValueError(f\"❌ Invalid model selected: {model_name}\")\n",
        "\n",
        "    # Get the absolute path to the current script's directory\n",
        "    # Instead of using __file__ (which is undefined in notebook),\n",
        "    # use os.getcwd() to get current working directory and then join\n",
        "    # with \"WhisperASR\" assuming the models are there.\n",
        "    current_dir = os.path.join(os.getcwd(), \"WhisperASR\")\n",
        "\n",
        "    # Construct the full path to the model file\n",
        "    model_path = os.path.join(current_dir, ALLOWED_MODELS[model_name])\n",
        "\n",
        "    print(f\"Checking if model weights exist at: {model_path}\")  # Debugging line\n",
        "\n",
        "    if not os.path.exists(model_path):\n",
        "        raise FileNotFoundError(f\"🚫 Model weights not found at {model_path}. Ensure you have saved them.\")\n",
        "\n",
        "    print(f\"📥 Loading {model_name} architecture...\")\n",
        "\n",
        "    # ⚡ Load model architecture (model structure)\n",
        "    model = whisper.load_model(model_name, download_root=\"models\")  # This loads the architecture only (not weights)\n",
        "\n",
        "    print(f\"🔄 Applying saved weights from {model_path}...\")\n",
        "\n",
        "    # ⚡ Apply local weights\n",
        "    state_dict = torch.load(model_path, map_location=\"cpu\")  # Load model weights from the local file\n",
        "    model.load_state_dict(state_dict, strict=False)  # Apply the weights\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "pPJlxs9ydU1j",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "25b15888-b113-45e1-9e57-652dc5a23ae3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing WhisperASR/model.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "%%writefile WhisperASR/app.py\n",
        "import gradio as gr\n",
        "import time\n",
        "from fpdf import FPDF\n",
        "from WhisperASR.model import load_whisper_model  # Import model loader\n",
        "import os\n",
        "\n",
        "# ✅ Allowed Whisper models\n",
        "ALLOWED_MODELS = [\"tiny\", \"base\", \"medium\"]\n",
        "\n",
        "# Define the path to the example audio directory\n",
        "example_audio_dir = os.path.join(os.path.dirname(__file__), \"example_audios\")\n",
        "\n",
        "# List the example files in the directory\n",
        "example_files = [[os.path.join(example_audio_dir, file)] for file in os.listdir(example_audio_dir) if file.endswith(\".wav\")]\n",
        "\n",
        "def transcribe_audio_file(audio_file_path, model=\"medium\"):\n",
        "    try:\n",
        "        whisper_model = load_whisper_model(model)  # Load model from model.py\n",
        "        result = whisper_model.transcribe(str(audio_file_path))\n",
        "        return result.get(\"text\", \"⚠️ No transcription available.\")\n",
        "    except Exception as e:\n",
        "        return f\"❌ Error: {str(e)}\"\n",
        "\n",
        "def transcribe_audio_with_time(audio_file_path, model=\"medium\"):\n",
        "    if not audio_file_path:\n",
        "        return \"⚠️ No audio file provided.\", 0, \"\"\n",
        "\n",
        "    start_time = time.time()\n",
        "    transcription = transcribe_audio_file(audio_file_path, model)\n",
        "    end_time = time.time()\n",
        "    transcription_time = end_time - start_time\n",
        "\n",
        "    formatted_output = f\"📝 Transcription:\\n\\n{transcription}\"\n",
        "    return formatted_output, round(transcription_time, 3), transcription\n",
        "\n",
        "# 📥 Generate TXT or PDF Report\n",
        "def clean_text(text):\n",
        "    return text.encode(\"ascii\", \"ignore\").decode()  # Remove non-ASCII characters\n",
        "\n",
        "def generate_report_file(text, file_format=\"txt\"):\n",
        "    filename = f\"structured_report.{file_format}\"\n",
        "    text = clean_text(text)  # Remove unsupported characters\n",
        "\n",
        "    if file_format == \"txt\":\n",
        "        with open(filename, \"w\", encoding=\"utf-8\") as file:\n",
        "            file.write(text)\n",
        "    elif file_format == \"pdf\":\n",
        "        pdf = FPDF()\n",
        "        pdf.add_page()\n",
        "        pdf.set_font(\"Arial\", size=12)\n",
        "        pdf.multi_cell(200, 10, text)\n",
        "        pdf.output(filename, \"F\")\n",
        "\n",
        "    return filename\n",
        "\n",
        "# 🎯 Full Processing Pipeline\n",
        "def process_audio(audio_file, model=\"medium\", file_format=\"txt\"):\n",
        "    transcription_output, transcription_time, transcription_text = transcribe_audio_with_time(audio_file, model)\n",
        "\n",
        "    if not transcription_text.strip():\n",
        "        return \"⚠️ No valid transcription available.\", 0, \"\", None\n",
        "\n",
        "    report_content = f\"Structured Report\\n\\n{transcription_text}\\n\\nProcessing Time: {transcription_time:.3f} seconds\"\n",
        "    report_file = generate_report_file(report_content, file_format)\n",
        "\n",
        "    return transcription_output, transcription_time, report_content, report_file\n",
        "\n",
        "# 🚀 Gradio Interface\n",
        "demo = gr.Interface(\n",
        "    fn=process_audio,\n",
        "    inputs=[\n",
        "        gr.Audio(\n",
        "            type=\"filepath\",\n",
        "            label=\"🎵 Upload Audio File\",\n",
        "            value=example_files,  # Provide a list of lists for example files\n",
        "            file_count=\"multiple\",  # Allow multiple files to be uploaded\n",
        "        ),\n",
        "        gr.Dropdown(choices=ALLOWED_MODELS, value=\"medium\", label=\"🤖 Select Whisper Model\"),\n",
        "        gr.Radio(choices=[\"txt\", \"pdf\"], value=\"txt\", label=\"📄 Report Format\"),\n",
        "    ],\n",
        "    outputs=[\n",
        "        gr.Textbox(label=\"📝 Transcribed Report\"),\n",
        "        gr.Number(label=\"⏳ Processing Time (seconds)\", precision=3),\n",
        "        gr.Textbox(label=\"📑 Structured Report\"),\n",
        "        gr.File(label=\"📥 Download Report\"),\n",
        "    ],\n",
        "    title=\"🎙️ AI-Powered Audio Report Generator\",\n",
        "    description=\"Upload an **audio file** 🎵, and this tool will transcribe it into a **structured report** 📑 available for **download** 📥 in TXT/PDF format.\\n\\n### How to Use:\\n1. **Upload an Audio File** 🎵: Click the 'Upload Audio File' button and select your audio file.\\n2. **Choose a Model** 🤖: Select from Tiny, Base, or Medium Whisper models for transcription.\\n3. **Select Report Format** 📄: Choose either TXT or PDF for the structured report.\\n4. **Process & Download** ⏳📥: The tool will generate a transcription, and you can download the structured report once completed.\",\n",
        "    theme=\"default\",\n",
        ")\n",
        "\n",
        "# 🚀 Launch App\n",
        "demo.launch(debug=True, share=True)\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "1ChnVQCpkmgP",
        "outputId": "c3be146b-53c1-44df-f59a-5e94b4fd0c2e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n%%writefile WhisperASR/app.py\\nimport gradio as gr\\nimport time\\nfrom fpdf import FPDF\\nfrom WhisperASR.model import load_whisper_model  # Import model loader\\nimport os\\n\\n# ✅ Allowed Whisper models\\nALLOWED_MODELS = [\"tiny\", \"base\", \"medium\"]\\n\\n# Define the path to the example audio directory\\nexample_audio_dir = os.path.join(os.path.dirname(__file__), \"example_audios\")\\n\\n# List the example files in the directory\\nexample_files = [[os.path.join(example_audio_dir, file)] for file in os.listdir(example_audio_dir) if file.endswith(\".wav\")]\\n\\ndef transcribe_audio_file(audio_file_path, model=\"medium\"):\\n    try:\\n        whisper_model = load_whisper_model(model)  # Load model from model.py\\n        result = whisper_model.transcribe(str(audio_file_path))\\n        return result.get(\"text\", \"⚠️ No transcription available.\")\\n    except Exception as e:\\n        return f\"❌ Error: {str(e)}\"\\n\\ndef transcribe_audio_with_time(audio_file_path, model=\"medium\"):\\n    if not audio_file_path:\\n        return \"⚠️ No audio file provided.\", 0, \"\"\\n\\n    start_time = time.time()\\n    transcription = transcribe_audio_file(audio_file_path, model)\\n    end_time = time.time()\\n    transcription_time = end_time - start_time\\n\\n    formatted_output = f\"📝 Transcription:\\n\\n{transcription}\"\\n    return formatted_output, round(transcription_time, 3), transcription\\n\\n# 📥 Generate TXT or PDF Report\\ndef clean_text(text):\\n    return text.encode(\"ascii\", \"ignore\").decode()  # Remove non-ASCII characters\\n\\ndef generate_report_file(text, file_format=\"txt\"):\\n    filename = f\"structured_report.{file_format}\"\\n    text = clean_text(text)  # Remove unsupported characters\\n\\n    if file_format == \"txt\":\\n        with open(filename, \"w\", encoding=\"utf-8\") as file:\\n            file.write(text)\\n    elif file_format == \"pdf\":\\n        pdf = FPDF()\\n        pdf.add_page()\\n        pdf.set_font(\"Arial\", size=12)\\n        pdf.multi_cell(200, 10, text)\\n        pdf.output(filename, \"F\")\\n\\n    return filename\\n\\n# 🎯 Full Processing Pipeline\\ndef process_audio(audio_file, model=\"medium\", file_format=\"txt\"):\\n    transcription_output, transcription_time, transcription_text = transcribe_audio_with_time(audio_file, model)\\n\\n    if not transcription_text.strip():\\n        return \"⚠️ No valid transcription available.\", 0, \"\", None\\n\\n    report_content = f\"Structured Report\\n\\n{transcription_text}\\n\\nProcessing Time: {transcription_time:.3f} seconds\"\\n    report_file = generate_report_file(report_content, file_format)\\n\\n    return transcription_output, transcription_time, report_content, report_file\\n\\n# 🚀 Gradio Interface\\ndemo = gr.Interface(\\n    fn=process_audio,\\n    inputs=[\\n        gr.Audio(\\n            type=\"filepath\",\\n            label=\"🎵 Upload Audio File\",\\n            value=example_files,  # Provide a list of lists for example files\\n            file_count=\"multiple\",  # Allow multiple files to be uploaded\\n        ),\\n        gr.Dropdown(choices=ALLOWED_MODELS, value=\"medium\", label=\"🤖 Select Whisper Model\"),\\n        gr.Radio(choices=[\"txt\", \"pdf\"], value=\"txt\", label=\"📄 Report Format\"),\\n    ],\\n    outputs=[\\n        gr.Textbox(label=\"📝 Transcribed Report\"),\\n        gr.Number(label=\"⏳ Processing Time (seconds)\", precision=3),\\n        gr.Textbox(label=\"📑 Structured Report\"),\\n        gr.File(label=\"📥 Download Report\"),\\n    ],\\n    title=\"🎙️ AI-Powered Audio Report Generator\",\\n    description=\"Upload an **audio file** 🎵, and this tool will transcribe it into a **structured report** 📑 available for **download** 📥 in TXT/PDF format.\\n\\n### How to Use:\\n1. **Upload an Audio File** 🎵: Click the \\'Upload Audio File\\' button and select your audio file.\\n2. **Choose a Model** 🤖: Select from Tiny, Base, or Medium Whisper models for transcription.\\n3. **Select Report Format** 📄: Choose either TXT or PDF for the structured report.\\n4. **Process & Download** ⏳📥: The tool will generate a transcription, and you can download the structured report once completed.\",\\n    theme=\"default\",\\n)\\n\\n# 🚀 Launch App\\ndemo.launch(debug=True, share=True)\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "from WhisperASR import app\n",
        "\n",
        "!python app.py\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "mlkLa1e1wsRs",
        "outputId": "a7c4e95e-e5ee-4638-f76c-98bc48d9adc7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nfrom WhisperASR import app\\n\\n!python app.py\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile WhisperASR/app.py\n",
        "import gradio as gr\n",
        "import time\n",
        "from fpdf import FPDF\n",
        "from WhisperASR.model import load_whisper_model  # Import model loader\n",
        "import os\n",
        "\n",
        "# ✅ Allowed Whisper models\n",
        "ALLOWED_MODELS = [\"tiny\", \"base\", \"medium\"]\n",
        "\n",
        "# Define the path to the example audio directory\n",
        "example_audio_dir = os.path.join(os.path.dirname(__file__), \"example_audios\")\n",
        "\n",
        "# List the example files in the directory (you can filter based on file extension, e.g., .wav)\n",
        "example_files = [os.path.join(example_audio_dir, file) for file in os.listdir(example_audio_dir) if file.endswith(\".wav\")]\n",
        "\n",
        "def transcribe_audio_file(audio_file_path, model=\"medium\"):\n",
        "    try:\n",
        "        whisper_model = load_whisper_model(model)  # Load model from model.py\n",
        "        result = whisper_model.transcribe(str(audio_file_path))\n",
        "        return result.get(\"text\", \"⚠️ No transcription available.\")\n",
        "    except Exception as e:\n",
        "        return f\"❌ Error: {str(e)}\"\n",
        "\n",
        "def transcribe_audio_with_time(audio_file_path, model=\"medium\"):\n",
        "    if not audio_file_path:\n",
        "        return \"⚠️ No audio file provided.\", 0, \"\"\n",
        "\n",
        "    start_time = time.time()\n",
        "    transcription = transcribe_audio_file(audio_file_path, model)\n",
        "    end_time = time.time()\n",
        "    transcription_time = end_time - start_time\n",
        "\n",
        "    formatted_output = f\"📝 Transcription:\\n\\n{transcription}\"\n",
        "    return formatted_output, round(transcription_time, 3), transcription\n",
        "\n",
        "# 📥 Generate TXT or PDF Report\n",
        "def clean_text(text):\n",
        "    return text.encode(\"ascii\", \"ignore\").decode()  # Remove non-ASCII characters\n",
        "\n",
        "def generate_report_file(text, file_format=\"txt\"):\n",
        "    filename = f\"structured_report.{file_format}\"\n",
        "    text = clean_text(text)  # Remove unsupported characters\n",
        "\n",
        "    if file_format == \"txt\":\n",
        "        with open(filename, \"w\", encoding=\"utf-8\") as file:\n",
        "            file.write(text)\n",
        "    elif file_format == \"pdf\":\n",
        "        pdf = FPDF()\n",
        "        pdf.add_page()\n",
        "        pdf.set_font(\"Arial\", size=12)\n",
        "        pdf.multi_cell(200, 10, text)\n",
        "        pdf.output(filename, \"F\")\n",
        "\n",
        "    return filename\n",
        "\n",
        "# 🎯 Full Processing Pipeline\n",
        "def process_audio(audio_file, model=\"medium\", file_format=\"txt\"):\n",
        "    transcription_output, transcription_time, transcription_text = transcribe_audio_with_time(audio_file, model)\n",
        "\n",
        "    if not transcription_text.strip():\n",
        "        return \"⚠️ No valid transcription available.\", 0, \"\", None\n",
        "\n",
        "    report_content = f\"Structured Report\\n\\n{transcription_text}\\n\\nProcessing Time: {transcription_time:.3f} seconds\"\n",
        "    report_file = generate_report_file(report_content, file_format)\n",
        "\n",
        "    return transcription_output, transcription_time, report_content, report_file\n",
        "\n",
        "# 🚀 Gradio Interface\n",
        "demo = gr.Interface(\n",
        "    fn=process_audio,\n",
        "    inputs=[\n",
        "        gr.Audio(\n",
        "            type=\"filepath\",\n",
        "            label=\"🎵 Upload Audio File\",\n",
        "            value=example_files[0] if example_files else None# Pre-select the first example file\n",
        "            #choices=example_files,  # Allow the user to select from the list of example files\n",
        "        ),\n",
        "        gr.Dropdown(choices=ALLOWED_MODELS, value=\"medium\", label=\"🤖 Select Whisper Model\"),\n",
        "        gr.Radio(choices=[\"txt\", \"pdf\"], value=\"txt\", label=\"📄 Report Format\"),\n",
        "    ],\n",
        "    outputs=[\n",
        "        gr.Textbox(label=\"📝 Transcribed Report\"),\n",
        "        gr.Number(label=\"⏳ Processing Time (seconds)\", precision=3),\n",
        "        gr.Textbox(label=\"📑 Structured Report\"),\n",
        "        gr.File(label=\"📥 Download Report\"),\n",
        "    ],\n",
        "    title=\"🎙️ AI-Powered Audio Report Generator\",\n",
        "    description=\"Upload an **audio file** 🎵, and this tool will transcribe it into a **structured report** 📑 available for **download** 📥 in TXT/PDF format.\\n\\n### How to Use:\\n1. **Upload an Audio File** 🎵: Click the 'Upload Audio File' button and select your audio file.\\n2. **Choose a Model** 🤖: Select from Tiny, Base, or Medium Whisper models for transcription.\\n3. **Select Report Format** 📄: Choose either TXT or PDF for the structured report.\\n4. **Process & Download** ⏳📥: The tool will generate a transcription, and you can download the structured report once completed.\",\n",
        "    theme=\"default\",\n",
        ")\n",
        "\n",
        "# 🚀 Launch App\n",
        "demo.launch(debug=True, share=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p9onYR7fw21x",
        "outputId": "e7010aa2-89ef-4ee9-8f29-d41677887517"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing WhisperASR/app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from WhisperASR import app\n",
        "\n",
        "!python app.py"
      ],
      "metadata": {
        "id": "a0IflBcm0fLU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 732
        },
        "outputId": "7c42b833-5246-4e2e-b9ce-a874391b8df5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://9bd98f4cb0b2b774de.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://9bd98f4cb0b2b774de.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checking if model weights exist at: /content/WhisperASR/models/whisper_tiny.pth\n",
            "📥 Loading tiny architecture...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|█████████████████████████████████████| 72.1M/72.1M [00:03<00:00, 19.9MiB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔄 Applying saved weights from /content/WhisperASR/models/whisper_tiny.pth...\n",
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://9bd98f4cb0b2b774de.gradio.live\n",
            "python3: can't open file '/content/app.py': [Errno 2] No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q pipreqs"
      ],
      "metadata": {
        "id": "Ilsyaqeomb0N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "project_dir = Path(\"WhisperASR\")\n",
        "project_dir"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-czCi96Fna6r",
        "outputId": "2aca3b57-1d97-4309-8146-bfa5dd4fbaf3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PosixPath('WhisperASR')"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pipreqs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lG5LnkGsmoZ4",
        "outputId": "813382eb-fcda-4eeb-c009-f8ca323e1537"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO: Not scanning for jupyter notebooks.\n",
            "WARNING: Import named \"gradio\" not found locally. Trying to resolve it at the PyPI server.\n",
            "WARNING: Import named \"gradio\" was resolved to \"gradio:5.22.0\" package (https://pypi.org/project/gradio/).\n",
            "Please, verify manually the final list of requirements.txt to avoid possible dependency confusions.\n",
            "INFO: Successfully saved requirements file in /content/requirements.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# zip recusively\n",
        "!zip -r WhisperASR.zip /WhisperASR * -x \"*.pyc\" \"*.ipynb\" \"*__pycache__*\" \"*ipynb_checkpoints*\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q5QF3un0cizw",
        "outputId": "6ff5d2be-1cf9-4b53-9343-27704e01ce6e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: WhisperASR/ (stored 0%)\n",
            "  adding: WhisperASR/model.py (deflated 54%)\n",
            "  adding: WhisperASR/app.py (deflated 60%)\n",
            "  adding: WhisperASR/models/ (stored 0%)\n",
            "  adding: WhisperASR/models/whisper_base.pth (deflated 42%)\n",
            "  adding: WhisperASR/models/whisper_medium.pth (deflated 41%)\n",
            "  adding: WhisperASR/models/whisper_tiny.pth (deflated 42%)\n",
            "  adding: WhisperASR/example_audios/ (stored 0%)\n",
            "  adding: WhisperASR/example_audios/1.wav (deflated 34%)\n",
            "  adding: WhisperASR/example_audios/5.wav (deflated 36%)\n",
            "  adding: WhisperASR/example_audios/4.wav (deflated 50%)\n",
            "  adding: dataset/ (stored 0%)\n",
            "  adding: dataset/photos/ (stored 0%)\n",
            "  adding: dataset/photos/3.jpg (deflated 8%)\n",
            "  adding: dataset/photos/4.jpg (deflated 16%)\n",
            "  adding: dataset/photos/2.jpg (deflated 8%)\n",
            "  adding: dataset/photos/1.jpg (deflated 4%)\n",
            "  adding: dataset/photos/5.jpg (deflated 8%)\n",
            "  adding: dataset/text/ (stored 0%)\n",
            "  adding: dataset/text/5.wav.txt (deflated 32%)\n",
            "  adding: dataset/text/3.wav.txt (deflated 30%)\n",
            "  adding: dataset/text/2.wav.txt (deflated 46%)\n",
            "  adding: dataset/text/1.wav.txt (deflated 33%)\n",
            "  adding: dataset/text/4.wav.txt (deflated 29%)\n",
            "  adding: dataset/photo_txt/ (stored 0%)\n",
            "  adding: dataset/photo_txt/5.jpg.txt (deflated 35%)\n",
            "  adding: dataset/photo_txt/1.jpg.txt (deflated 34%)\n",
            "  adding: dataset/photo_txt/3.jpg.txt (deflated 43%)\n",
            "  adding: dataset/photo_txt/2.jpg.txt (deflated 54%)\n",
            "  adding: dataset/photo_txt/4.jpg.txt (deflated 27%)\n",
            "  adding: dataset/audio/ (stored 0%)\n",
            "  adding: dataset/audio/1.wav (deflated 34%)\n",
            "  adding: dataset/audio/3.wav (deflated 50%)\n",
            "  adding: dataset/audio/5.wav (deflated 36%)\n",
            "  adding: dataset/audio/4.wav (deflated 50%)\n",
            "  adding: dataset/audio/2.wav (deflated 49%)\n",
            "  adding: dataset.zip (stored 0%)\n",
            "  adding: models/ (stored 0%)\n",
            "  adding: models/tiny.pt (deflated 9%)\n",
            "  adding: sample_data/ (stored 0%)\n",
            "  adding: sample_data/README.md (deflated 39%)\n",
            "  adding: sample_data/anscombe.json (deflated 83%)\n",
            "  adding: sample_data/california_housing_train.csv (deflated 79%)\n",
            "  adding: sample_data/mnist_test.csv (deflated 88%)\n",
            "  adding: sample_data/mnist_train_small.csv (deflated 88%)\n",
            "  adding: sample_data/california_housing_test.csv (deflated 76%)\n"
          ]
        }
      ]
    }
  ]
}